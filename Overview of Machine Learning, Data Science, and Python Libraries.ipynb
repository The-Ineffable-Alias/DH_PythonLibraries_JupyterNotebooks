{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of some of the Python packages.\n",
    "\n",
    "***Disclaimer: I am using the words library, module, and package very loosely. And to be honest it is difficult to keep the distictions entirely clear. Just we aware that I am not applying these terms by their true definition. For our purposes the distinctions are largely insignificant.***\n",
    "\n",
    "So what is this module/package/library business. To be clear for once let's lay the defitions out and then promptly forget them.\n",
    "<dl>\n",
    "    <dt>Module</dt>\n",
    "    <dd>Simply a python file which contains python functions, global variables etc. It is an easy way to store and execute functions and tasks across python programs.</dd>\n",
    "    <dt>Package</dt>\n",
    "    <dd>A package is a collection of modules in a directory. No real difference from a Library that often is interfaced with through an API. They are both just more robust collections of modules and packages</dd>\n",
    "    <dt>Library</dt>\n",
    "    <dd>The library is not a strict unit in Python but is also used to describe a container for multiple packages/modules in a directory that often is interfaced with through an API. No real difference from a Package. They are both just more robust collections of modules and packages</dd>\n",
    "<dl>\n",
    "\n",
    "## Resources:\n",
    "[Python Data Analysis Library](https://pandas.pydata.org/)\n",
    "<br/>\n",
    "[Pandas Documentation](http://pandas.pydata.org/pandas-docs/stable/)\n",
    "<br/>A python tool for creating and anlaysing data structures.\n",
    "<br/>\n",
    "[Numpy Documentation](http://www.numpy.org/)<br/>\n",
    "NumPy is the fundamental package for scientific computing with Python. Basically numpy is used for math and working wiht arrays.\n",
    "[Scikit Learn Documentation](http://scikit-learn.org/stable/index.html)<br/>\n",
    "Pythn tooks for basic machine learning, data mining, and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy and Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we can use numpy to generate a random array of integers and then use Pandas to put those in a data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.random.randint(0,100,size=(60, 8))\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can flatten the array, sommetimes useful in NLP.\n",
    "r = np.ravel(values)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from the array\n",
    "data = pd.DataFrame(values,columns=['A','B','C','D','E','F','G','H'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas and NumPy provide us with a myriad of ways to change, manipulate, analyze, and visualize data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='scatter',x='A',y='B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px\"/>\n",
    "# Scikit Learn\n",
    "\n",
    "Machine leanring is just an algoritm that a computer uses to make generalizations about data.\n",
    "\n",
    "* [scikit-learn](http://scikit-learn.org/stable/)<br/>\n",
    "This is and open source package built on several libraries: NumPy, SciPy, and matplotlib\n",
    "\n",
    "## The Machine Learning Process\n",
    "1. Define the problem and indetify the research question.\n",
    "2. Find the Data\n",
    "3. Prepare the data (munging)\n",
    "4. Select the algorithm\n",
    "5. Train the model\n",
    "6. Test the model\n",
    "7. Communicate/share results\n",
    "\n",
    "## Types of Machine Learning algorithms:\n",
    "* Classification: assign some data to a class. Is email spam.\n",
    "* Regression: predict a value based on a set of features: based on location an dsize what is the price of a house.\n",
    "* Clustering: looking for patterns in data. Categorizing text\n",
    "* Rule Extraction: Recommendation engines. Given a set of data and some rules we can make a prediction.\n",
    "\n",
    "### Example of machine learning with classification\n",
    "Is a flower a daisy or a rose. \n",
    "The corpus is the set of data we have that gives us the information the algorithm will use to learn how to distinguish the characteristics of daisies and roses.\n",
    "\n",
    "We need to choose and Algorithm.\n",
    "\n",
    "When the algorithm is chosen we train it on a set of training data from the corpus. E.g. white pedals = daisy, red = rose.\n",
    "\n",
    "Input vectors are the color of the pedal. The output we desire is what type of flower we get.\n",
    "\n",
    "\n",
    "\n",
    "## Supervised and Unsupervised learning\n",
    "### supervised\n",
    "These algorithms that have lables associated with the training data that are used to correct and tune the model. For example, we have location and size data for a house and we want to predict the price. We know the labels (price and size) and we want to learn how the predict cost. Using the data corpus we can adjust the model so that it can predict a target variable (e.g. price). \n",
    "\n",
    "This data set has a set of feature vectors or variables, the x variables. \n",
    "\n",
    "The attribute that we want to predict is called the label, or the y variable.\n",
    "\n",
    "\n",
    "### unsupervised\n",
    "The model is set up to learn the patterns and structures in the data when we do not know what the correlations might be. Given a large set of data these algorithms look for relationships that are not apparent.\n",
    "\n",
    "We have no labels, we do not know what our y variable is, and we only have the x variables. From this data unsupervised algorithms look for patterns.\n",
    "\n",
    "Clustering is a common type of unsupervised machine learning, which is a methof of identifying groups of similar content after an analysis of a selected group of features.\n",
    "\n",
    "## Data types\n",
    "### Continuous\n",
    "\n",
    "This is data that has an infinite set of values in a range. These are values like salary, height, weight, age.\n",
    "\n",
    "### Categorical Data\n",
    "This is data that has a limited number of values. These are values like Month, color, day of the week, yes/no.\n",
    "\n",
    "This type of data canot necessarily be ordered.\n",
    "\n",
    "\n",
    "## Scaling Data\n",
    "This is also called normalization. The purpose is to standardized the variability in feature details. The exam data below has student scores and it is not clear if they are scores out of 100, out of 120, etc. To work with this data we need to standardize it. \n",
    "\n",
    "## Feature Vectors\n",
    "\n",
    "First know that a vector is simply a list of numbers (1,2,5,3). To visualize what this means for machine learning it helps to think of a 2-dimensional vector that can be plotted on a cartesian plane. \n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/vector2D.png\" style=\"margin-top:2em;width:40%\"/>\n",
    "</div>\n",
    "\n",
    "Vectors can contain a number of numbers, but it is difficult for us to visualize such a complicated system of values.\n",
    "\n",
    "In machine learning each measurement we have is a feature. You can think of this as the columns in a table. If we are trying to determing the cost of a house, some features we might consider are the number of bedrooms, the square footage, the number of bathrooms, etc. These data points are our features and a collection of these data points for one house would be the feature vector for that house. \n",
    "\n",
    "The key is to identify features that can correlate to the target variable you want to predict. To continue the house example, including the weight of the home owner would probably not be a useful feature for determining the proce of a house, while looking at the size of the house in square feet might be.\n",
    "\n",
    "This is where domain knowledge is very valuable for identifying useful features. A environmental scientist could be better suited for identifying useful features when trying to predict climate change than a software developer. \n",
    "\n",
    "However, another way to look at feature determination is to look at it from a mathematical point of view and see what features correlate mathematically to the target variable best. This is often a useful approach and one that different algorithms rely on as well.\n",
    "\n",
    "The combination of these two approaches is often the most effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Exercise - feature extraction: let's make some features\n",
    "\n",
    "Let's look at some student exam data and see how we can scale some of the variables and transform some of the data into useful features (aka **feature extraction**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we bring the data into the notebook and save it as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know anything about this data except what we have in front of us in this instance. This may often be the case. Be careful to avoid assumptions. The score numbers look like they are percentages, but we have no way of confirming this. These could be points and we have no access to the scale. Scikit learn provides a means to standardize these data based on mean standard deviation.\n",
    "\n",
    "In the cell below we will scale the data and assign the results back to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "test_data[['math score']] = preprocessing.scale(test_data[['math score']])\n",
    "test_data[['reading score']] = preprocessing.scale(test_data[['reading score']])\n",
    "test_data[['writing score']] = preprocessing.scale(test_data[['writing score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding for categorical data\n",
    "This identifies a value and uses a 0 or 1 to indicate if it is part of a category or not. We can do this type of categorization easily with scikit learn's LabelEncoder. It can generate this type of encoding automatically for a column. \n",
    "\n",
    "Gender is a perfect column for this type of feature extraction. A computer can do nothing with the values \"female/male\" (e.g. what is \"male\" + \"male\"?). However a computer can evaluate the number 1 or 0. So we can use 0 t indicate \"female\" and 1 to indicate \"male\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This let's the LabelEncoder know what type of data it is going to be handling. \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(test_data['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes reflect the labels of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the LabelEncoder to replace the data inthe gender column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['gender'] = le.transform(test_data['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also has a great way to do this kind of \"one hot encoding\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will return a new dataframe of the one hot data.\n",
    "pd.get_dummies(exam_data['parental level of education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put this in our original test_data dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.get_dummies(test_data, columns=['parental level of education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the \"test preparation course\" column and see what values we have there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can this also be one hot encoded.\n",
    "test_data['test preparation course'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the rest of our columns to one hot encoding data so the computer can make sense of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.get_dummies(test_data, columns=['lunch','test preparation course'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a collection of features that can be processed mathematically by a computer. \n",
    "\n",
    "### Data visualization and Exploration\n",
    "\n",
    "If we plot out the dat for the scores we can see that there seems to be a strong corelation between reading score and writing score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "test_data.plot(kind='scatter',x='reading score',y='math score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.plot(kind='scatter',x='reading score',y='writing score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['gender'].value_counts().plot(kind='pie', x='lunch_standard', label=\"female = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Using numeric data to analyze text\n",
    "So what does this mean for text data?\n",
    "\n",
    "How can we use numeric data to analyze text. Let's think about this in the context of a sentiment analyzer for a review. Imagine our review of the movie is: \"This was the best movie!\"\n",
    "\n",
    "We can imagine our `text = 'This was the best movie!'`\n",
    "\n",
    "Well we can tokenize it to `tokens = ['This', 'was', 'the', 'best', 'movie']`\n",
    "\n",
    "We could use one hot method on the set of vocabulary so that if a sentence has certain words we can esatblish a correlation. For example, the `tokens` document would become something like `[('This',1),('was',1), ('the',1), ('best',1), ('movie',1)].`\n",
    "\n",
    "Probelms with this is that it does not count frequency if we reduce its vocabulary to generate the one hot integers. It also doe not take into account order. And as the corpus grows its vocabulary the matrix becomes enormous, more like a sparse matrix. This is not a very intelligent way to represent the sentence information.\n",
    "\n",
    "A better method is Frequence based embeddings. \n",
    "\n",
    "### Frequency based embeddings\n",
    "These rely on count (how many times a word has occured in a document) and Term Frequency - Inverse Document Frequency(TF-IDF)\n",
    "#### Count\n",
    "If we see that the word 'bad' occurs more frequently in a review then we can say the movie is reviewed negatively. \n",
    "\n",
    "Problems arise here agian:\n",
    "* If the corpus is very large then again we have a huge set of feature vectors (the vocabulary). \n",
    "* We are still losing the context of the words.\n",
    "* Semantics and relationships are not preserved\n",
    "\n",
    "Some solutions to deal with a large vocabulary could be to limit the number of words we choose. We could rely on the top 50 words in our word count but we have seen before that this favors words that might not convey any useful information (e.g. \"a\", \"the\", \"an\"). To remedy this we may filter out stopwords that do not affect the sentiment of a review.\n",
    "\n",
    "Another method is to hash words to buckets to reduce the vocabulary size. This means if we have 10,000 words we can create 8,000 buckets. We need to be careful to not create buckets that minimize the amount of collisions, that is we don't have two words that map to the same bucket. For example we could put the words \"actor\" and \"actress\" into one bucket if we are trying to evalute if a review is describing good or bad acting. THese two words are now treated as one and counted as one. So the the sentence \"the actor was great and the actress was unknown\" would get 2 counts for the \"actor, actress\" bucket. We would never know that these are separate words however in our count, only that this bucket has a higher count than other buckets.\n",
    "\n",
    "#### TF-IDF (term frequency - inverse document frequency)\n",
    "This is a measure of how often a word occurs in a document compared to how often it occurs in a corpus.\n",
    "\n",
    "TF-IDF looks at how often a term appears in a single document. So the more often a word appears in the document it is likely to be considered important. This is considered against how often a word appears in the corpus. So the logic is that a word that appears more frequently in a document but less frequently in the corpus is a significant term for that document. \n",
    "\n",
    "This has the advantage of reducing the feature vector size and giving relevance to frequency in a document. However we still have the draw back that the context of the word is not captured.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing data (aka turning text into numbers)\n",
    "We will start with a very small corpus of data so we can see what is going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small corpus of four documents.\n",
    "corpus = ['The first wish I have is for a dog. I love dogs.',\n",
    "         'The second wish I have is for a cat.',\n",
    "         'Is the third wish on my wish list a polar bear? Is it?',\n",
    "         'Number four of the things I want is a cow. Number four, cow. Yup.',\n",
    "         'The last thing I ever want is a hot dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using the Notebooks `shift + tab` feature to learn more about how the countVectorizer works. also note we are taking it from a submodule of Scikit Learn called `feature_extraction.text`. This submidule provides tools for building feature vectors from text documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple frequency based vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "word_count = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we have creates a sparse matrix. This is just a matrix that contains a lot of empty, or 0, vlaues, which is often the case when working with word counts in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will show the matrix broken up by document, word represented by an integer and word count. \n",
    "# Note that the order of the array does not reflect the order of the documents in the corpus.\n",
    "\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If we take one of these lines, `(4,26)     1`, 4 indicates the document (remember not necessarily the 4th documentas our corpus is laid out), 26 is the word id, and the trailing 1 is the count. </p>\n",
    "<p>Let's take an even closer look at what is in this matrix.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the id of a word:\n",
    "vectorizer.vocabulary_.get('wish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the entire vocabulary and the ids\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice 'dog' and 'dogs' are counted separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the feature names.\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's look at this information as data in a data frame using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_data = pd.DataFrame(word_count.toarray(), columns=vectorizer.get_feature_names())\n",
    "corpus_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could do some data exploration with pandas if we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have enough data here but if we had several more hundreds of rows we might want to use unsupervised learning to look for patterns in these \"documents\". Hopefuly we would see that the alogrithm would group them into wishes for dogs, wish for cats, and wishes for cows or bears. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer\n",
    "\n",
    "Now lets work with the TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(corpus)\n",
    "word_tfidf = tfidf.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will see a simlar matrix.\n",
    "# In this case each vocabulary word gets a td-idf score in stead of a count.\n",
    "print(word_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access this matrix much in the same way we accessed the countVectorizer matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_data = pd.DataFrame(word_tdidf.toarray(), columns=tdidf.get_feature_names())\n",
    "corpus_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have a large vocabulary but imagine we do. Maybe we would want to reduce the feature count we could hash the features. Scikit Learn also gives us a hashing tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# here n_features is n number of buckets\n",
    "hashing = HashingVectorizer(n_features=18)\n",
    "word_hash = hashing.fit_transform(corpus)\n",
    "print(word_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now where we had our word count up to 27, now we only have 18 values (0-17) and the frequency is not represented as a count but as some normalized data. One downside to hashing is that we have lost the link back to the vocabulary, so tracing back is not possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_hash.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_hash.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_data = pd.DataFrame(word_hash.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
